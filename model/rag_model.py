"""
RAG ëª¨ë¸ ëª¨ë“ˆ
ë©€í‹° ì¿¼ë¦¬ ìƒì„± ë° ReRank ê¸°ëŠ¥ì„ í¬í•¨í•œ RAG ì‹œìŠ¤í…œ
"""

import os
from typing import Optional
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from pydantic import BaseModel, Field
from sentence_transformers import CrossEncoder

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class QueryGeneration(BaseModel):
    """ì¿¼ë¦¬ ìƒì„± ëª¨ë¸"""
    queries: list[str] = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡")


class RAGModel:
    """ë©€í‹° ì¿¼ë¦¬ ë° ReRank ê¸°ëŠ¥ì„ í¬í•¨í•œ RAG ëª¨ë¸"""
    
    def __init__(
        self,
        chromadb_path: str= "./chroma_data" ,
        collection_name: str = "medical_qa",
        embedding_model: str = "text-embedding-3-small",
        llm_model: str = "gpt-5-mini",
        temperature: float = 0,
        retrieval_k: int = 10,
        rerank_top_n: int = 8,
        rerank_model_name: str = "zeroentropy/zerank-2"
    ):
        """
        RAG ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            chromadb_path: ChromaDB ë°ì´í„° ê²½ë¡œ
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model: Embedding ëª¨ë¸ ì´ë¦„
            llm_model: LLM ëª¨ë¸ ì´ë¦„
            temperature: LLM temperature
            retrieval_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
            rerank_top_n: ReRank í›„ ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            rerank_model_name: ReRank ëª¨ë¸ ì´ë¦„
        """
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.hf_token = os.getenv("HF_TOKEN")
        
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # Embedding ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=self.openai_api_key
        )
        
        # ChromaDB ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vectorstore = Chroma(
            persist_directory=chromadb_path,
            collection_name=collection_name,
            embedding_function=self.embeddings
        )
        
        # Retriever ì´ˆê¸°í™”
        self.retriever = self.vectorstore.as_retriever(
            search_type='similarity',
            search_kwargs={'k': retrieval_k}
        )
        
        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model=llm_model,
            temperature=temperature,
            openai_api_key=self.openai_api_key
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt = ChatPromptTemplate.from_template('''
ë‹¤ìŒ ë¬¸ë§¥ë§Œ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ë¬¸ë§¥ : """
{context}
"""
ì§ˆë¬¸ : {question}
''')
        
        # ë©€í‹° ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.query_generation_prompt = ChatPromptTemplate.from_template("""\
ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ
3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ì„± ê²€ìƒ‰ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ê´€ì ì„ ì œê³µí•˜ëŠ”ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤

ì§ˆë¬¸ :{question}
""")
        
        # ë©€í‹° ì¿¼ë¦¬ ìƒì„± ì²´ì¸
        self.query_generation_chain = (
            self.query_generation_prompt
            | self.llm.with_structured_output(QueryGeneration)
            | (lambda x: x.queries)
        )
        
        # ReRank ëª¨ë¸ ì´ˆê¸°í™” (ì´ˆê¸°í™” ì‹œì ì— ë¯¸ë¦¬ ë¡œë“œ)
        self.rerank_top_n = rerank_top_n
        self.rerank_model_name = rerank_model_name
        
        # HF_TOKENì´ ìˆìœ¼ë©´ ì‚¬ìš©
        if self.hf_token:
            os.environ["HF_TOKEN"] = self.hf_token
        
        # ReRank ëª¨ë¸ì„ ì´ˆê¸°í™” ì‹œì ì— ë¯¸ë¦¬ ë¡œë“œ (í•œ ë²ˆë§Œ ë¡œë“œë˜ê³  ì¬ì‚¬ìš©)
        print("ğŸ”„ ReRank ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.rerank_model = CrossEncoder(
            self.rerank_model_name,
            trust_remote_code=True
        )
        print("âœ… ReRank ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ReRank ì²´ì¸ì„ ì´ˆê¸°í™” ì‹œì ì— ë¯¸ë¦¬ ìƒì„± (ì¬ì‚¬ìš©)
        self.rerank_chain = (
            {
                "question": RunnablePassthrough(),
                "docs": self.query_generation_chain | self.retriever.map(),
            }
            | RunnableLambda(lambda x: self._rerank_topn(x, top_n=self.rerank_top_n))
        )
        
        # ë©€í‹° ì¿¼ë¦¬ + ReRank RAG ì²´ì¸ (í•­ìƒ ReRank ì‚¬ìš©)
        self.rag_chain = {
            "question": RunnablePassthrough(),
            "context": self.rerank_chain,
        } | self.prompt | self.llm | StrOutputParser()
    
    def _flatten_dedup(self, docs_nested):
        """
        ì¤‘ì²©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í‰íƒ„í™”í•˜ê³  ì¤‘ë³µ ì œê±°
        
        Args:
            docs_nested: list[list[Document]] ë˜ëŠ” list[Document]
        
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        if not docs_nested:
            return []
        
        if isinstance(docs_nested[0], Document):
            flat = docs_nested
        else:
            flat = [d for sub in docs_nested for d in sub]
        
        # ì¤‘ë³µ ì œê±°(í…ìŠ¤íŠ¸ ê¸°ì¤€)
        seen = set()
        uniq = []
        for d in flat:
            key = d.page_content
            if key not in seen:
                seen.add(key)
                uniq.append(d)
        return uniq
    
    def _rerank_topn(self, payload, top_n: Optional[int] = None):
        """
        ë¬¸ì„œë¥¼ ReRankí•˜ì—¬ ìƒìœ„ Nê°œ ë°˜í™˜
        
        Args:
            payload: {"question": str, "docs": list[list[Document]] ë˜ëŠ” list[Document]}
            top_n: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: self.rerank_top_n)
        
        Returns:
            ReRankëœ Document ë¦¬ìŠ¤íŠ¸
        """
        if top_n is None:
            top_n = self.rerank_top_n
        
        q = payload["question"]
        docs = self._flatten_dedup(payload["docs"])
        
        if not docs:
            return []
        
        # ReRank ëª¨ë¸ì€ ì´ë¯¸ ì´ˆê¸°í™” ì‹œì ì— ë¡œë“œë¨
        scored = []
        for d in docs:
            # batch size 1ë¡œ í˜¸ì¶œ (pad_token ì˜¤ë¥˜ íšŒí”¼)
            s = float(self.rerank_model.predict([(q, d.page_content)])[0])
            scored.append((s, d))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [d for _, d in scored[:top_n]]
    
    def query(
        self,
        question: str,
        config: Optional[dict] = None
    ) -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í•­ìƒ ReRank ì‚¬ìš©)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            config: LangChain ì‹¤í–‰ ì„¤ì • (ì„ íƒì‚¬í•­)
        
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        # ì§ˆë¬¸ ì²˜ë¦¬ (í•­ìƒ ReRank í¬í•¨)
        if config:
            result = self.rag_chain.invoke(question, config=config)
        else:
            result = self.rag_chain.invoke(question)
        
        return result


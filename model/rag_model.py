"""
RAG ëª¨ë¸ ëª¨ë“ˆ
ë©€í‹° ì¿¼ë¦¬ ìƒì„± ë° ReRank ê¸°ëŠ¥ì„ í¬í•¨í•œ RAG ì‹œìŠ¤í…œ
"""

import os
import shutil
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv
from langchain_chroma import Chroma
# from langchain_openai import OpenAIEmbeddings  # HuggingFace ì„ë² ë”© ì‚¬ìš©ìœ¼ë¡œ ì£¼ì„ì²˜ë¦¬
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from pydantic import BaseModel, Field
from sentence_transformers import CrossEncoder
from huggingface_hub import hf_hub_download
import torch

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


def ensure_hf_chroma_assets(
    persist_dir: str | Path,
    repo_id: str,
    repo_type: str = "dataset",
    sqlite_filename: str = "chroma.sqlite3",
    zip_filename: str = "ea99f6f7-3cf3-4ae6-8b86-dcabc4b70a9c-20260120T081052Z-1-001.zip",
    token: Optional[str] = None
) -> Path:
    """
    HF Hubì—ì„œ chroma.sqlite3 + zip íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í›„ persist_dirì— ë°°ì¹˜
    ê¸°ì¡´ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ
    
    Args:
        persist_dir: ChromaDB ë°ì´í„°ë¥¼ ì €ì¥í•  ê²½ë¡œ
        repo_id: í—ˆê¹…í˜ì´ìŠ¤ ë¦¬í¬ì§€í† ë¦¬ ID
        repo_type: ë¦¬í¬ì§€í† ë¦¬ íƒ€ì… (ê¸°ë³¸ê°’: "dataset")
        sqlite_filename: SQLite íŒŒì¼ëª…
        zip_filename: ZIP íŒŒì¼ëª…
        token: HF í† í° (private ë¦¬í¬ì§€í† ë¦¬ì¸ ê²½ìš° í•„ìš”)
    
    Returns:
        persist_dirì˜ Path ê°ì²´
    """
    persist_dir = Path(persist_dir).resolve()
    persist_dir.mkdir(parents=True, exist_ok=True)

    # ì‹¤ì œ ì•±ì´ ì°¸ì¡°í•  ìœ„ì¹˜
    sqlite_dst = persist_dir / sqlite_filename
    zip_dst = persist_dir / zip_filename

    # ê¸°ì¡´ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì‚­ì œ
    if sqlite_dst.exists():
        print(f"ğŸ—‘ï¸  ê¸°ì¡´ {sqlite_filename} ì‚­ì œ ì¤‘...")
        sqlite_dst.unlink()
    
    if zip_dst.exists():
        print(f"ğŸ—‘ï¸  ê¸°ì¡´ {zip_filename} ì‚­ì œ ì¤‘...")
        zip_dst.unlink()

    # HF ìºì‹œì— ë‹¤ìš´ë¡œë“œ (ê²½ë¡œ ë°˜í™˜)
    print(f"ğŸ“¥ {sqlite_filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
    sqlite_cache = Path(hf_hub_download(
        repo_id=repo_id,
        repo_type=repo_type,
        filename=sqlite_filename,
        token=token,
    ))
    print(f"ğŸ“¥ {zip_filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
    zip_cache = Path(hf_hub_download(
        repo_id=repo_id,
        repo_type=repo_type,
        filename=zip_filename,
        token=token,
    ))

    # ì‹¤ì œ ì•±ì´ ì°¸ì¡°í•  ìœ„ì¹˜ë¡œ "ë³µì‚¬" (ìºì‹œ íŒŒì¼ ì§ì ‘ ìˆ˜ì •/ì´ë™ ë°©ì§€)
    shutil.copy2(sqlite_cache, sqlite_dst)
    shutil.copy2(zip_cache, zip_dst)
    
    print(f"âœ… {sqlite_filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    print(f"âœ… {zip_filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

    return persist_dir


def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.cuda.is_available():
        return "cuda"       # NVIDIA GPU
    elif torch.backends.mps.is_available():
        return "mps"        # Mac Apple Silicon 
    else:
        return "cpu"        # CPU


class QueryGeneration(BaseModel):
    """ì¿¼ë¦¬ ìƒì„± ëª¨ë¸"""
    queries: list[str] = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡")


class RAGModel:
    """ë©€í‹° ì¿¼ë¦¬ ë° ReRank ê¸°ëŠ¥ì„ í¬í•¨í•œ RAG ëª¨ë¸"""
    
    def __init__(
        self,
        chromadb_path: str = "./chromaDB_data",
        collection_name: str = "med_knowledge",
        embedding_model: str = "BAAI/bge-m3",
        llm_model: str = "gpt-5-mini",
        temperature: float = 0,
        retrieval_k: int = 10,
        rerank_top_n: int = 3,
        rerank_model_name: str = "zeroentropy/zerank-2",
        hf_repo_id: str = "yj512/likelion_project2_chromadb",
        hf_repo_type: str = "dataset",
        sqlite_filename: str = "chroma.sqlite3",
        zip_filename: str = "ea99f6f7-3cf3-4ae6-8b86-dcabc4b70a9c-20260120T081052Z-1-001.zip",
        download_from_hf: bool = True
    ):
        """
        RAG ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            chromadb_path: ChromaDB ë°ì´í„° ê²½ë¡œ
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model: Embedding ëª¨ë¸ ì´ë¦„ (HuggingFace ëª¨ë¸)
            llm_model: LLM ëª¨ë¸ ì´ë¦„
            temperature: LLM temperature
            retrieval_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
            rerank_top_n: ReRank í›„ ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            rerank_model_name: ReRank ëª¨ë¸ ì´ë¦„
            hf_repo_id: í—ˆê¹…í˜ì´ìŠ¤ ë¦¬í¬ì§€í† ë¦¬ ID
            hf_repo_type: í—ˆê¹…í˜ì´ìŠ¤ ë¦¬í¬ì§€í† ë¦¬ íƒ€ì…
            sqlite_filename: SQLite íŒŒì¼ëª…
            zip_filename: ZIP íŒŒì¼ëª…
            download_from_hf: í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.hf_token = os.getenv("HF_TOKEN")
        
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # HF_TOKENì´ ìˆìœ¼ë©´ ì‚¬ìš©
        if self.hf_token:
            os.environ["HF_TOKEN"] = self.hf_token
        
        # í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ChromaDB íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        if download_from_hf:
            print("ğŸ”„ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ChromaDB íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            ensure_hf_chroma_assets(
                persist_dir=chromadb_path,
                repo_id=hf_repo_id,
                repo_type=hf_repo_type,
                sqlite_filename=sqlite_filename,
                zip_filename=zip_filename,
                token=self.hf_token
            )
            print("âœ… ChromaDB íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = get_device()
        print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
        
        # Embedding ëª¨ë¸ ì´ˆê¸°í™” (HuggingFace ì„ë² ë”© ì‚¬ìš©)
        print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ({embedding_model}) ë¡œë”© ì¤‘...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # # OpenAI ì„ë² ë”© ëª¨ë¸ (ì£¼ì„ì²˜ë¦¬)
        # self.embeddings = OpenAIEmbeddings(
        #     model=embedding_model,
        #     openai_api_key=self.openai_api_key
        # )
        
        # ChromaDB ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vectorstore = Chroma(
            persist_directory=chromadb_path,
            collection_name=collection_name,
            embedding_function=self.embeddings
        )
        
        # Retriever ì´ˆê¸°í™”
        self.retriever = self.vectorstore.as_retriever(
            search_type='similarity',
            search_kwargs={'k': retrieval_k}
        )
        
        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model=llm_model,
            temperature=temperature,
            openai_api_key=self.openai_api_key
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt = ChatPromptTemplate.from_template('''
ë‹¤ìŒ ë¬¸ë§¥ë§Œ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ë¬¸ë§¥ : """
{context}
"""
ì§ˆë¬¸ : {question}
''')
        
        # ë©€í‹° ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.query_generation_prompt = ChatPromptTemplate.from_template("""\
ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ
3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ì„± ê²€ìƒ‰ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ê´€ì ì„ ì œê³µí•˜ëŠ”ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤

ì§ˆë¬¸ :{question}
""")
        
        # ë©€í‹° ì¿¼ë¦¬ ìƒì„± ì²´ì¸
        self.query_generation_chain = (
            self.query_generation_prompt
            | self.llm.with_structured_output(QueryGeneration)
            | (lambda x: x.queries)
        )
        
        # ReRank ëª¨ë¸ ì´ˆê¸°í™” (ì´ˆê¸°í™” ì‹œì ì— ë¯¸ë¦¬ ë¡œë“œ)
        self.rerank_top_n = rerank_top_n
        self.rerank_model_name = rerank_model_name

        # ğŸ”§ ReRank ëª¨ë¸ lazy loading (ì²˜ìŒ rerankê°€ ì‹¤ì œë¡œ í•„ìš”í•´ì§ˆ ë•Œ ë¡œë“œ)
        self._rerank_model = None
        
        # ReRank ì²´ì¸ì„ ì´ˆê¸°í™” ì‹œì ì— ë¯¸ë¦¬ ìƒì„± (ì¬ì‚¬ìš©)
        self.rerank_chain = (
            {
                "question": RunnablePassthrough(),
                "docs": self.query_generation_chain | self.retriever.map(),
            }
            | RunnableLambda(lambda x: self._rerank_topn(x, top_n=self.rerank_top_n))
        )
        
        # ë©€í‹° ì¿¼ë¦¬ + ReRank RAG ì²´ì¸ (í•­ìƒ ReRank ì‚¬ìš©)
        self.rag_chain = {
            "question": RunnablePassthrough(),
            "context": self.rerank_chain,
        } | self.prompt | self.llm | StrOutputParser()
    
    def _flatten_dedup(self, docs_nested):
        """
        ì¤‘ì²©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í‰íƒ„í™”í•˜ê³  ì¤‘ë³µ ì œê±°
        
        Args:
            docs_nested: list[list[Document]] ë˜ëŠ” list[Document]
        
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        if not docs_nested:
            return []
        
        if isinstance(docs_nested[0], Document):
            flat = docs_nested
        else:
            flat = [d for sub in docs_nested for d in sub]
        
        # ì¤‘ë³µ ì œê±°(í…ìŠ¤íŠ¸ ê¸°ì¤€)
        seen = set()
        uniq = []
        for d in flat:
            key = d.page_content
            if key not in seen:
                seen.add(key)
                uniq.append(d)
        return uniq

    def _get_rerank_model(self):
        """ReRank ëª¨ë¸ì„ í•„ìš” ì‹œì ì— 1íšŒ ë¡œë“œí•´ì„œ ì¬ì‚¬ìš©"""
        if self._rerank_model is None:
            device = get_device()
            print("ğŸ”„ ReRank ëª¨ë¸ lazy loading...")
            # CrossEncoderëŠ” ë‚´ë¶€ì ìœ¼ë¡œ HF ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©° deviceëŠ” 'cpu'/'cuda' ë¬¸ìì—´ì„ ë°›ìŒ
            # CUDA í™˜ê²½ì´ë©´ float16 ì‚¬ìš©ì„ ì‹œë„í•˜ê³ , ì•„ë‹ˆë©´ ê¸°ë³¸ dtypeìœ¼ë¡œ ë‘ 
            model_kwargs = {"torch_dtype": torch.float16} if device == "cuda" else {}
            self._rerank_model = CrossEncoder(
                self.rerank_model_name,
                trust_remote_code=True,
                device=device,
                model_kwargs=model_kwargs,
            )
            print("âœ… ReRank ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        return self._rerank_model
    
    def _rerank_topn(self, payload, top_n: Optional[int] = None):
        """
        ë¬¸ì„œë¥¼ ReRankí•˜ì—¬ ìƒìœ„ Nê°œ ë°˜í™˜
        
        Args:
            payload: {"question": str, "docs": list[list[Document]] ë˜ëŠ” list[Document]}
            top_n: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: self.rerank_top_n)
        
        Returns:
            ReRankëœ Document ë¦¬ìŠ¤íŠ¸
        """
        if top_n is None:
            top_n = self.rerank_top_n
        
        q = payload["question"]
        docs = self._flatten_dedup(payload["docs"])
        
        if not docs:
            return []

        reranker = self._get_rerank_model()
        scored = []
        for d in docs:
            # batch size 1ë¡œ í˜¸ì¶œ (pad_token ì˜¤ë¥˜ íšŒí”¼)
            s = float(reranker.predict([(q, d.page_content)])[0])
            scored.append((s, d))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [d for _, d in scored[:top_n]]
    
    def query(
        self,
        question: str,
        config: Optional[dict] = None
    ) -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í•­ìƒ ReRank ì‚¬ìš©)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            config: LangChain ì‹¤í–‰ ì„¤ì • (ì„ íƒì‚¬í•­)
        
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        # ì§ˆë¬¸ ì²˜ë¦¬ (í•­ìƒ ReRank í¬í•¨)
        if config:
            return self.rag_chain.invoke(question, config=config)
        return self.rag_chain.invoke(question)

